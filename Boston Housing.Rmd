---
title: "5052 Statistical Machine Learning Project"
author: "Kah Meng Soh"
date: '2022-04-24'
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(glmnet)
data=read.csv('C:/Users/micke/Desktop/Boston.csv')
data_scaled <- cbind(scale(data[,1:13]),data[,14])
xfull <- data[,1:13]
yfull <- data[,14]
set.seed(1)
size <- floor(0.8 *  nrow(data_scaled))
trainset <- sample(seq_len(nrow(data_scaled)), size = size)
train <- data_scaled[trainset, ]
xtrain <- train[,1:13]
ytrain <- train[,14]
test <- data_scaled[-trainset,]
xtest <- test[,1:13]
ytest <- test[,14]
```
```{r}
#Ridge Regression (alpha=0)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(xtrain, ytrain, alpha = 0, lambda = grid)
plot(ridge.mod)
set.seed (1)
cv.out <- cv.glmnet(xtrain, ytrain, alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.pred <- predict(ridge.mod , s = bestlam,newx = xtest)
mean (( ridge.pred - ytest)^2)
coef <- glmnet(xfull, yfull, alpha = 0)
predict(coef , type = "coefficients", s = bestlam)[1:14, ]
```
```{r}
#Lasso Regression (alpha=1)
lasso.mod <- glmnet(xtrain, ytrain, alpha = 1, lambda = grid)
plot(lasso.mod)
set.seed (1)
cv.out <- cv.glmnet(xtrain, ytrain, alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso.pred <- predict(lasso.mod , s = bestlam,newx = xtest)
mean (( lasso.pred - ytest)^2)
coef <- glmnet(xfull, yfull, alpha = 1)
predict(coef , type = "coefficients", s = bestlam)[1:14, ]
```
```{r}
#OLS Regression
train=data.frame(train)
test=data.frame(test)
ols.mod = lm(V14~.,train)
ols.pred= predict(ols.mod, newdata=test)
mean (( ols.pred - ytest)^2)
ols.modfull = lm(MEDV~.,data)
summary(ols.modfull)
```
```{r}
#Elastic-Net Regression (alpha=0.25)
en.mod <- glmnet(xtrain, ytrain, alpha = 0.25, lambda = grid)
plot(en.mod)
set.seed (1)
cv.out <- cv.glmnet(xtrain, ytrain, alpha = 0.25)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
en.pred <- predict(en.mod , s = bestlam,newx = xtest)
mean (( en.pred - ytest)^2)
coef <- glmnet(xfull, yfull, alpha = 0.25)
predict(coef , type = "coefficients", s = bestlam)[1:14, ]
```
```{r}
#Elastic-Net Regression (alpha=0.5)
en.mod <- glmnet(xtrain, ytrain, alpha = 0.5, lambda = grid)
plot(en.mod)
set.seed (1)
cv.out <- cv.glmnet(xtrain, ytrain, alpha = 0.5)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
en.pred <- predict(en.mod , s = bestlam,newx = xtest)
mean (( en.pred - ytest)^2)
coef <- glmnet(xfull, yfull, alpha = 0.5)
predict(coef , type = "coefficients", s = bestlam)[1:14, ]
```
```{r}
#Elastic-Net Regression (alpha=0.75)
en.mod <- glmnet(xtrain, ytrain, alpha = 0.75, lambda = grid)
plot(en.mod)
set.seed (1)
cv.out <- cv.glmnet(xtrain, ytrain, alpha = 0.75)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
en.pred <- predict(en.mod , s = bestlam,newx = xtest)
mean (( en.pred - ytest)^2)
coef <- glmnet(xfull, yfull, alpha = 0.75)
predict(coef , type = "coefficients", s = bestlam)[1:14, ]
```
```{r fig.height=7.5,fig.width=7.5}
# To study the linearity assumption of OLS we will look at OLS although lasso regression is better.
#MSE of OLS 14.50773 is too high compared to the sample mean of response 22.532806, maybe the data isn't fit for linear regression, let's see if some underlying assumption of OLS violated.
par(mfrow = c(2, 2))
plot(ols.modfull)
#There is banana shape in the residual vs fitted plot suggesting non-linearity of data, let's try boxcox transformation just to be sure.
par(mfrow = c(1, 1))
library(MASS)
bc=boxcox(ols.modfull)
i=which.max(bc$y)
bc$x[i]
#Suggested transformation is power of close to 0, which is log transformation. 
ols.modfull = lm(log(MEDV)~.,data)
par(mfrow = c(2, 2))
plot(ols.modfull)
#The transformed model still failed the non-linearity assumption, further suggesting that the true underlying model is non-linear.
```